# ğŸ“˜ Lecture: Recurrent Neural Networks (RNN)

---

## 1. Introduction to RNN

* **Recurrent Neural Networks (RNNs)** are a class of neural networks specifically designed for **sequential and temporal data**.
* Unlike feedforward networks, RNNs **retain information from previous steps** through hidden states.
* Useful when **context and order matter** (e.g., language, speech, stock trends).

ğŸ‘‰ Example:

* Feedforward NN predicting image = "cat" or "dog" â†’ order doesnâ€™t matter.
* Predicting the next word in â€œI am going to the â€¦â€ â†’ order **does** matter.

---

## 2. Historical Context

* **1980sâ€“1990s**: Early RNN models by **Jordan** and **Elman**.
* **1997**: Hochreiter & Schmidhuber proposed **LSTM**, solving vanishing gradients.
* **2010s**: RNNs widely used in NLP and speech.
* **Today**: Transformers dominate NLP, but RNNs remain strong for **time-series, real-time, and edge applications**.

---

## 3. Types of Artificial Neural Networks

* **Feedforward Neural Network (FNN)** â†’ Input â†’ Hidden â†’ Output (no memory).
* **Convolutional Neural Network (CNN)** â†’ Best for spatial data (images).
* **Recurrent Neural Network (RNN)** â†’ Best for sequential/temporal data.

---

## 4. Sequential vs Non-Sequential Data

* **Non-sequential data**: Independent samples.

  * Example: Image classification.
* **Sequential data**: Order matters, past influences future.

  * Examples: Speech, text, time-series.

---

## 5. Why RNN is Different

* Traditional NN processes each input **independently**.
* RNN introduces **feedback loops**:

  * Stores past context in **hidden state**.
  * Can handle **variable-length sequences**.
* Learns **temporal dependencies**.

ğŸ‘‰ Example:

* Sentiment depends on sequence:

  * â€œI love this movieâ€ â†’ positive.
  * â€œI donâ€™t love this movieâ€ â†’ negative (requires context memory).

---

## 6. Applications of RNN

* **Natural Language Processing (NLP)**: Machine translation, text generation, sentiment analysis.
* **Speech Recognition**: Converting audio â†’ text.
* **Time-Series Forecasting**: Stock market, weather prediction.
* **Music & Text Generation**: Create melodies, poems, or stories.
* **Healthcare**: Predict patient health from sequential data.
* **Video Analysis**: Captioning, action recognition.

---

## 7. Architecture of RNN

### Components

1. **Input Layer** â†’ Sequential data ((x_t)).
2. **Hidden Layer** â†’ Combines (x_t) + previous hidden state (h_{t-1}).
3. **Output Layer** â†’ Produces prediction ((y_t)).

### Equations

* Hidden state update:
  [
  h_t = f(W_h h_{t-1} + W_x x_t + b)
  ]
* Output:
  [
  y_t = g(W_y h_t + c)
  ]

Where:

* (f) = activation function (tanh/ReLU).
* (g) = output activation (softmax/sigmoid/linear).

---

## 8. Working of RNN

1. First input (x_1) â†’ hidden state (h_1).
2. Second input (x_2) + (h_1) â†’ hidden state (h_2).
3. Repeat for all steps in sequence.
4. Output at each step (many-to-many) or final step (many-to-one).

ğŸ‘‰ Example:

* Machine translation â†’ output at every time step.
* Sentiment analysis â†’ output only at last step.

---

## 9. Variants of RNN

* **Vanilla RNN** â†’ Basic model, suffers from vanishing gradients.
* **LSTM (Long Short-Term Memory)** â†’ Has input, output, forget gates.
* **GRU (Gated Recurrent Unit)** â†’ Simplified LSTM.
* **Bidirectional RNN** â†’ Processes sequence in both directions.
* **Deep RNN** â†’ Stacked multiple layers for complexity.

---

## 10. Training RNN â€“ Backpropagation Through Time (BPTT)

* **Unroll RNN** across time steps.
* Apply backpropagation to update weights.
* **Problems**:

  * Vanishing gradients (info lost over long sequences).
  * Exploding gradients (unstable updates).
* **Solutions**: Gradient clipping, LSTM, GRU.

---

## 11. Activation Functions in RNN

* **Hidden state**: tanh, ReLU, sigmoid.
* **Output layer**:

  * Softmax â†’ multi-class classification.
  * Sigmoid â†’ binary classification.
  * Linear â†’ regression tasks.

ğŸ‘‰ Example:

* Sentiment analysis â†’ Softmax.
* Stock price prediction â†’ Linear.

---

## 12. Loss Functions in RNN

* **Classification**: Cross-Entropy Loss (categorical/binary).
* **Sequence Prediction**: Cross-Entropy (token-by-token).
* **Regression/Forecasting**: MSE, MAE.

ğŸ‘‰ Example:

* Next word prediction â†’ Cross-Entropy.
* Predicting temperature â†’ MSE.

---

## 13. Layers in RNN

* **Input layer**: One-hot encoding or Embedding.
* **Hidden layer**: SimpleRNN, LSTM, GRU.
* **Stacked RNN layers**: For deeper learning.
* **Dropout layers**: Prevent overfitting.
* **Output layer**: Dense with activation (sigmoid/softmax/linear).

ğŸ‘‰ Example (Keras):

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense

model = Sequential()
model.add(Embedding(input_dim=5000, output_dim=128))
model.add(SimpleRNN(128, activation='tanh'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
```

---

## 14. Optimization in RNN

* **Optimizers**: SGD, Adam (most common), RMSProp (good for sparse updates).
* **Regularization**: Dropout, weight decay, gradient clipping.

---

## 15. Evaluation Metrics

* **Classification**: Accuracy, Precision, Recall, F1.
* **Sequence tasks**: Perplexity, BLEU score.
* **Regression**: RMSE, MAE, RÂ².

---

## 16. Real-World Example

### Task: Sentiment Analysis using RNN

* **Input**: Movie reviews.
* **Process**:

  * Convert words â†’ embeddings.
  * RNN processes words sequentially.
  * Final hidden state â†’ sentiment prediction.
* **Loss**: Binary Cross-Entropy.
* **Activation**: Sigmoid (positive/negative).
* **Output**: Sentiment score (0 = negative, 1 = positive).

ğŸ‘‰ Example Sentence:

* â€œThe movie was not good.â€

  * RNN remembers **â€œnotâ€** before **â€œgoodâ€** â†’ predicts negative sentiment.

---

## 17. Advantages & Limitations

âœ… Advantages:

* Captures temporal dependencies.
* Works with variable-length input.
* Strong in sequence-based tasks.

âŒ Limitations:

* Vanishing/exploding gradients.
* Struggles with long-term dependencies.
* Slow to train on long sequences.
* Replaced in NLP by Transformers.

---

## 18. Future Directions

* Still used in **time-series forecasting, speech recognition, IoT**.
* In NLP, **Transformers (BERT, GPT)** are preferred.
* RNNs remain useful where **efficiency and low-latency** are needed.

---

## 19. Summary

* RNNs â†’ Designed for sequential data, with hidden state memory.
* Key components â†’ Input, hidden, output layers.
* Training â†’ Backpropagation Through Time (BPTT).
* Variants â†’ LSTM, GRU, Bidirectional RNN.
* Loss functions & activations depend on task type.
* Applications â†’ NLP, speech, forecasting, healthcare, video analysis.
* Future â†’ Transformers dominate, but RNNs still important in real-time systems.

---
